{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desenvolvimento de conversao arquivo p05 para netcdf\n",
    "Este é um esqueleto para processamento dos arquivos csv/txt das plataformas, e conversão para netcdf, assim como preenchimento dos dados de informações (metadados). Os passos são os seguintes:\n",
    "1. Abre arquivo JSON onde informações de configuração, da plataforma e dos arquivo netcdf estão armazenadas. Isso é necessário para flexibilizar o código e permitir reutilizacao do código. Isso é vantajoso quando várias estações com mesmo modelo de arquivo de saída são utilizadas.\n",
    "2. Abertura do arquivo de imagem e conversão para base64. O arquivo netcdf não tem um tipo de variável adequado para armazenamento direto de arquivos binários. Ainda não esta fechado a melhor forma de armazenar esse tipo de dado dentro do netcdf, mas acho que converter o arquivo para baser64 (texto, muito utilizado em navegadores de internet) é um método razoavel. A vantagem dessa conversão é permitr armazenar os arquivos como texto simples, e fácil de recuperar. Esse formato ainda pode ser revisto.\n",
    "3. Leitura do arquivo de dados, e varredura para alguns termos para confirmar que arquivo é da estação definida. Leitura das medidas em uma tabela Pandas\n",
    "4. Criação do arquivo netcdf\n",
    "5. Geração de metadados\n",
    "6. Inserção dos dados no arquivo netcdf\n",
    "7. Dump do arquivo para verificação\n",
    "8. Verificação do padrão CF\n",
    "\n",
    "Após desenvolvimento é possível exportar o codigo para .py e utilizar o codigo diretamente na linha de comando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega bibliotecas de acordo com o necessario\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "from netCDF4 import Dataset,num2date, date2num\n",
    "from datetime import timezone, timedelta\n",
    "\n",
    "from netCDF4 import Dataset,num2date, date2num, stringtoarr\n",
    "import json\n",
    "\n",
    "from gbdhidro import utilconversor\n",
    "from gbdhidro import utilcf\n",
    "from gbdhidro.netcdfjson import NetCDFJSON\n",
    "import base64\n",
    "\n",
    "import argparse\n",
    "\n",
    "from io import BytesIO\n",
    "import logging\n",
    "\n",
    "# Inicia logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Jupyter detectado. Alterando modo de operacao para DEBUG\n"
     ]
    }
   ],
   "source": [
    "# Verifica se esta no jupyter. Isso altera o comportamento do codigo\n",
    "IN_JUPYTER = utilconversor.isnotebook()\n",
    "if IN_JUPYTER:\n",
    "    DEBUG = True\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger.info(\"Jupyter detectado. Alterando modo de operacao para DEBUG\")\n",
    "else:\n",
    "    DEBUG = False\n",
    "    logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "if IN_JUPYTER:\n",
    "    from PIL import Image\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo de erro utilizado no shell em caso de problema\n",
    "ERROR_CODE = 1\n",
    "\n",
    "if not IN_JUPYTER:\n",
    "    parser = argparse.ArgumentParser(description='Converte arquivo para netCDF.')\n",
    "    parser.add_argument(\"-i\", \"--input\", help=\"nome do arquivo de entrada\")\n",
    "    parser.add_argument(\"-j\", \"--json\", help=\"nome do arquivo de configuracao json\")\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"nome do arquivo de saida. Se nao for informado eh gerado automaticamente\")\n",
    "    parser.add_argument(\"-d\", \"--directory\", help=\"nome do diretorio de saida\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    OUTPUT_FOLDER = args.directory\n",
    "    OUTPUT_FILE = args.output\n",
    "    FILE_PATH = args.input\n",
    "    JSON_FILE = args.json\n",
    "    \n",
    "    if FILE_PATH is None:\n",
    "        parser.print_help()\n",
    "        exit(ERROR_CODE)\n",
    "        \n",
    "    # TODO: encontrar uma forma para carreagr o arquivo automatico\n",
    "    #if JSON_FILE is None:\n",
    "    #    parser.print_help()\n",
    "    #    exit(ERROR_CODE)\n",
    "\n",
    "else:\n",
    "    # esta dentro do jupyter, utiliza valores padroes para debug\n",
    "    FILE_PATH = 'm01.txt'\n",
    "    OUTPUT_FOLDER = '.'\n",
    "    OUTPUT_FILE = None\n",
    "    JSON_FILE = 'm01.json'\n",
    "    \n",
    "# Arquivo com configuracao da estacao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre arquivo json de configuração\n",
    "with open(JSON_FILE, 'r') as fp:\n",
    "    json_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_JUPYTER:\n",
    "    display(json_data['station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do arquivo de dados\n",
    "STATION_NAME = json_data['station']['name']\n",
    "# String com identificador utilizado para certificar que eh\n",
    "# o arquivo da estao escolhida\n",
    "STATION_ID = json_data['station']['name_match_string']\n",
    "LATITUDE = json_data['station']['latitude']\n",
    "LONGITUDE = json_data['station']['longitude']\n",
    "ALTITUDE = json_data['station']['altitude']\n",
    "IMAGE_FILE = json_data['station']['image']\n",
    "DATETIME_MATCH = json_data['station']['datetime_match_string']\n",
    "PRECIPITATION_MATCH = json_data['station']['precipitation_match_string']\n",
    "\n",
    "# Seleciona encoding do arquivo\n",
    "ENCODING = json_data['station']['input_file_encoding']\n",
    "DECIMAL = json_data['station']['decimal_separator']\n",
    "SEPARATOR = json_data['station']['column_delimiter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Conversao para estacao {}\".format(STATION_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le arquivo de imagem\n",
    "with open(IMAGE_FILE, \"rb\") as image_file:\n",
    "    image_base64 = utilcf.bin2base64(image_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open(FILE_PATH, \"r\")\n",
    "first_line = fo.readline()\n",
    "second_line = fo.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(first_line)\n",
    "logger.debug(second_line)\n",
    "first_line_sep = first_line.split(SEPARATOR)\n",
    "second_line_sep = second_line.split(SEPARATOR)\n",
    "header = []\n",
    "for l1, l2 in zip(first_line_sep, second_line_sep):\n",
    "    header.append(\"{} {}\".format(l1.strip(),l2.strip()))\n",
    "logger.debug(header)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procura por alguns termos especifcos para verificar se o arquivo\n",
    "# esta dentro do esperado\n",
    "if utilconversor.find_matches(\" \".join(header),STATION_ID):\n",
    "    logger.info('Processando estacao ' + STATION_NAME)\n",
    "else:\n",
    "    logger.info(\"Erro: não é arquivo da estacao \" + STATION_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai dados da aquisicao\n",
    "table = pd.read_csv(fo, sep=SEPARATOR, skiprows=0, verbose=False, na_filter=True, header=None, encoding=ENCODING, decimal=DECIMAL, warn_bad_lines=True)\n",
    "fo.close()\n",
    "\n",
    "table.columns = header\n",
    "\n",
    "if IN_JUPYTER:\n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procura por colunas de dados e gera erro se não encontrar  \n",
    "cols = {\n",
    "        \"date\": {\"match\":[\"Date\"], \"col\": None},\n",
    "        \"time\": {\"match\":[\"Time\"], \"col\": None},\n",
    "        \"temp_out\": {\"match\":[\"Temp\", \"Out\"], \"col\": None},\n",
    "        \"hi_temp\": {\"match\":[\"Hi\",\"Temp\"], \"col\": None},\n",
    "        \"low_temp\": {\"match\":[\"Low\",\"Temp\"], \"col\": None},\n",
    "        \"out_hum\": {\"match\":[\"Out\",\"Hum\"], \"col\": None},\n",
    "        \"dew_point\": {\"match\":[\"Dew\",\"Pt\"], \"col\": None},\n",
    "        \"wind_speed\": {\"match\":[\"Wind\",\"Speed\"], \"col\": None},\n",
    "        \"wind_dir\": {\"match\":[\"Wind\",\"Dir\"], \"col\": None},\n",
    "        \"wind_run\": {\"match\":[\"Wind\",\"Run\"], \"col\": None},\n",
    "        \"wind_hi_speed\": {\"match\":[\"Hi\",\"Speed\"], \"col\": None},\n",
    "        \"wind_hi_dir\": {\"match\":[\"Hi\",\"Dir\"], \"col\": None},\n",
    "        \"wind_chill\": {\"match\":[\"Wind\",\"Chill\"], \"col\": None},\n",
    "        \"heat_index\": {\"match\":[\"heat\",\"Index\"], \"col\": None},\n",
    "        \"thw_index\": {\"match\":[\"THW\",\"Index\"], \"col\": None},\n",
    "        \"bar\": {\"match\":[\"bar\"], \"col\": None},\n",
    "        \"rain\": {\"match\":[\"rain\"], \"col\": None},\n",
    "        \"rain_rate\": {\"match\":[\"rain\", \"Rate\"], \"col\": None},\n",
    "        \"solar_rad\": {\"match\":[\"Solar\", \"Rad.\"], \"col\": None},\n",
    "        \"solar_energy\": {\"match\":[\"Solar\", \"Energy\"], \"col\": None},\n",
    "        \"hi_solar_rad\": {\"match\":[\"Hi\", \"Solar\", \"rad\"], \"col\": None},\n",
    "        \"uv_index\": {\"match\":[\"uv\", \"index\"], \"col\": None},\n",
    "        \"uv_dose\": {\"match\":[\"uv\", \"dose\"], \"col\": None},\n",
    "        \"hi_uv\": {\"match\":[\"hi\", \"uv\"], \"col\": None},\n",
    "        \"heat_d_d\": {\"match\":[\"heat\", \"D-D\"], \"col\": None},\n",
    "        \"cool_d_d\": {\"match\":[\"cool\", \"D-D\"], \"col\": None},\n",
    "        \"in_temp\": {\"match\":[\"in\", \"temp\"], \"col\": None},\n",
    "        \"in_hum\": {\"match\":[\"in\", \"hum\"], \"col\": None},\n",
    "        \"in_dew\": {\"match\":[\"in\", \"dew\"], \"col\": None},\n",
    "        \"in_heat\": {\"match\":[\"in\", \"heat\"], \"col\": None},\n",
    "        \"et\": {\"match\":[\"ET\"], \"col\": None},\n",
    "        \"wind_samp\": {\"match\":[\"wind\", \"samp\"], \"col\": None},\n",
    "        \"wind_tx\": {\"match\":[\"wind\", \"tx\"], \"col\": None},\n",
    "        \"iss_recept\": {\"match\":[\"iss\", \"recept\"], \"col\": None},\n",
    "        \"arc_int\": {\"match\":[\"arc\", \"int\"], \"col\": None}\n",
    "    }\n",
    "\n",
    "var_list = {\n",
    "        \"date\": {\"match\":[\"Date\"], \"pandas_col\": None},\n",
    "        \"time\": {\"match\":[\"Time\"], \"pandas_col\": None},\n",
    "        \"temperature\": {'netcdf_var': 'temperature', \"match\":[\"Temp\", \"Out\"], \"pandas_col\": None},    \n",
    "        \"temperature_high\": {'netcdf_var': 'temperature_high', \"match\":[\"Hi\", \"Temp\"], \"pandas_col\": None},\n",
    "        \"temperature_low\": {'netcdf_var': 'temperature_low', \"match\":[\"Low\", \"Temp\"], \"pandas_col\": None},\n",
    "        \"humidity\": {'netcdf_var': 'humidity', \"match\":[\"Out\",\"Hum\"], \"pandas_col\": None},\n",
    "        \"dew_point\": {'netcdf_var': 'dew_point', \"match\":[\"Dew\",\"Pt\"], \"pandas_col\": None},\n",
    "        \"wind_speed\": {'netcdf_var': 'wind_speed', \"match\":[\"Wind\",\"Speed\"], \"pandas_col\": None},\n",
    "        \"wind_speed_high\": {'netcdf_var': 'wind_speed_high', \"match\":[\"Hi\",\"Speed\"], \"pandas_col\": None},\n",
    "        \"air_pressure\": {'netcdf_var': 'air_pressure', \"match\":[\" Bar\"], \"pandas_col\": None},\n",
    "        'precipitation': {'netcdf_var':'precipitation', 'match':[' Rain'], 'pandas_col': None},\n",
    "        'precipitation_rate': {'netcdf_var':'precipitation_rate', 'match':['Rain', 'Rate'], 'pandas_col': None},\n",
    "        'solar_irradiance': {'netcdf_var':'solar_irradiance', 'match':['Solar', 'Rad'], 'pandas_col': None},\n",
    "        'evapotranspiration': {'netcdf_var':'evapotranspiration', 'match':[' ET'], 'pandas_col': None},\n",
    "    }\n",
    "\n",
    "\n",
    "for key, value in var_list.items():\n",
    "    found = False\n",
    "    for column in table.columns:\n",
    "        if utilconversor.find_matches(column,value['match']):\n",
    "            value['pandas_col'] = column\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        logger.error('Erro, nao foi encontrada coluna {}'.format(key))\n",
    "        exit(ERROR_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = table[var_list[\"date\"][\"pandas_col\"]]\n",
    "time = table[var_list[\"time\"][\"pandas_col\"]]\n",
    "# Apaga estes elementos da lista pois ja foram utilizados\n",
    "# evita atrapalhar algm codigo mais adiante\n",
    "del var_list['date']\n",
    "del var_list['time']\n",
    "\n",
    "date_str = date.str.cat(time, sep=\" \")\n",
    "date_time = pd.to_datetime(date_str, format='%d/%m/%y %H:%M')\n",
    "\n",
    "if IN_JUPYTER:\n",
    "    display(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processa datetime para incluir timezone\n",
    "#date_str = table[date_time_col]\n",
    "#date_time = pd.to_datetime(date_str, format='%d/%m/%y %I:%M:%S %p')\n",
    "# extrai informacao de gmt\n",
    "#gmt_hour_offset, gmt_minute_offset = utilconversor.get_gmt_offset(date_time.name)\n",
    "# TODO: Como converte para UTC essa estacao?\n",
    "gmt_hour_offset = 0\n",
    "gmt_minute_offset = 0\n",
    "tzinfo=timezone(timedelta(hours=gmt_hour_offset, minutes=gmt_minute_offset))\n",
    "# gera os indices com informacao de fuso horarios incluido\n",
    "index = date_time.dt.tz_localize(tzinfo)\n",
    "# converte para UTC\n",
    "index_utc = index.dt.tz_convert('UTC')\n",
    "first_day_str = utilcf.datetime2str(index_utc.iloc[0])\n",
    "last_day_str = utilcf.datetime2str(index_utc.iloc[-1])\n",
    "logger.info(\"Inicio e fim de medidas em UTC: {} - {}\".format(first_day_str, last_day_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra coluna com chuva\n",
    "#precipitation = table[cols['rain']['col']]\n",
    "\n",
    "#if IN_JUPYTER:\n",
    "#    display(precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera nome de arquivo de saida\n",
    "#file_name = '{}_{}_{}.nc'.format(STATION_NAME, first_day_str, last_day_str)\n",
    "#logger.info(\"Nome de arquivo de saida: {}\".format(file_name))\n",
    "\n",
    "if OUTPUT_FILE is None:\n",
    "    file_name = '{}_{}_{}.nc'.format(STATION_NAME, first_day_str, last_day_str)\n",
    "else:\n",
    "    file_name = OUTPUT_FILE\n",
    "\n",
    "if OUTPUT_FOLDER is None:\n",
    "    nc_file_path = file_name\n",
    "else:\n",
    "    nc_file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "\n",
    "logger.info(\"Nome de arquivo de saida: {}\".format(file_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geracao de arquivo netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc_file_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "# Cria arquivo netCDF\n",
    "nc_file = NetCDFJSON()\n",
    "nc_file.write(nc_file_path)\n",
    "# Le arquivo json com configuracao da estrutura do netcdf\n",
    "nc_file.load_json(JSON_FILE)\n",
    "nc_file.create_from_json()\n",
    "\n",
    "# pega handlers para dimensoes\n",
    "timeDim = nc_file.get_dimension('time')\n",
    "nameDim = nc_file.get_dimension('name_strlen')\n",
    "# pega handlers para variaveis\n",
    "time = nc_file.get_variable('time')\n",
    "time_bnds = nc_file.get_variable('time_bnds')\n",
    "lat = nc_file.get_variable('lat')\n",
    "lon = nc_file.get_variable('lon')\n",
    "alt = nc_file.get_variable('alt')\n",
    "station_name = nc_file.get_variable('station_name')\n",
    "precip = nc_file.get_variable('precipitation')\n",
    "station_image = nc_file.get_variable('station_image')\n",
    "\n",
    "# Recupera qual o valor uilizado para valores que estao faltando\n",
    "#FILL_VALUE = precip._FillValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insere lista de variaveis no netcdf\n",
    "for key, value in var_list.items():\n",
    "    nc_var = nc_file.get_variable(value['netcdf_var'])\n",
    "    FILL_VALUE = nc_var._FillValue\n",
    "    data_var = table[value['pandas_col']]\n",
    "    data_var = data_var.replace(np.nan, FILL_VALUE)\n",
    "    data_var = data_var.to_numpy()\n",
    "    nc_var[:] = data_var\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitui Nan por FILL_VALUE\n",
    "#precipitation = precipitation.replace(np.nan, FILL_VALUE)\n",
    "\n",
    "# Seta variaveis\n",
    "nc_time = index_utc.to_numpy()\n",
    "nc_time = date2num(nc_time,units=time.units,calendar=time.calendar)\n",
    "# Como a precipitacao e o acumulado entre a ultima medida e a atual\n",
    "# é necessario informar isso atraves dos bounds de tempo, ou seja,\n",
    "# os valores inferioes e superiores respectivos ao limite do eixo de temp0\n",
    "# O bound superior e o mesmo que o horario da medids\n",
    "nc_superior_bound_time = nc_time\n",
    "# bound inferior e o horario da ultima medida\n",
    "nc_inferior_bound_time = np.roll(nc_superior_bound_time,1)\n",
    "# a primeira medida não tem medida anterior, por isso seta para o mesmo valor\n",
    "nc_inferior_bound_time[0] = nc_superior_bound_time[0]\n",
    "# combina bound inferior com bound superior\n",
    "bnds = np.stack((nc_inferior_bound_time, nc_superior_bound_time), axis=-1)\n",
    "\n",
    "#nc_serie = precipitation.to_numpy()\n",
    "\n",
    "nc_station_name = STATION_NAME\n",
    "latitude = LATITUDE\n",
    "longitude = LONGITUDE\n",
    "altitude = ALTITUDE\n",
    "\n",
    "lat[:] = np.array([latitude])\n",
    "lon[:] = np.array([longitude])\n",
    "alt[:] = np.array([altitude])\n",
    "\n",
    "time[:] = nc_time\n",
    "time_bnds[:] = bnds\n",
    "\n",
    "#precip[:] = np.array(nc_serie)\n",
    "\n",
    "station_name[:] = stringtoarr(nc_station_name, nameDim.size)\n",
    "station_image[:] = stringtoarr(image_base64, len(image_base64))\n",
    "station_image.file_name = IMAGE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz processamento para metadados\n",
    "# Processa os dados ja convertidos para facilitar reutilizar o codigo depois\n",
    "\n",
    "# min max lat e lon\n",
    "min_lat = np.amin(lat)\n",
    "max_lat = np.amax(lat)\n",
    "min_lon = np.amin(lon)\n",
    "max_lon = np.amax(lon)\n",
    "\n",
    "# time duration\n",
    "min_time = num2date(np.amin(time), units=time.units, calendar=time.calendar)\n",
    "max_time = num2date(np.amax(time), units=time.units, calendar=time.calendar)\n",
    "min_time_str = utilcf.datetime2str(min_time)\n",
    "max_time_str = utilcf.datetime2str(max_time)\n",
    "time_delta = max_time - min_time\n",
    "time_delta_str = utilcf.timedelta2str(time_delta)\n",
    "\n",
    "# time resolution\n",
    "time1 = num2date(time[1], units=time.units, calendar=time.calendar)\n",
    "time0 = num2date(time[0], units=time.units, calendar=time.calendar)\n",
    "time_resolution = time1 - time0\n",
    "time_resolution_str = utilcf.timedelta2str(time_resolution)\n",
    "\n",
    "\n",
    "logger.info('Min/Max latitude: {}/{}'.format(min_lat,max_lat))\n",
    "logger.info('Min/Max longitude: {}/{}'.format(min_lon,max_lon))\n",
    "logger.info('Min/Max datetime: {}/{}'.format(min_time_str,max_time_str))\n",
    "logger.info('Time duration:{}'.format(time_delta_str))\n",
    "logger.info('Time resolution:{}'.format(time_resolution_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualiza metadados\n",
    "nc_file.rootgrp.geospatial_lat_min = min_lat\n",
    "nc_file.rootgrp.geospatial_lat_max = max_lat\n",
    "nc_file.rootgrp.geospatial_lon_min = min_lon\n",
    "nc_file.rootgrp.geospatial_lon_max = max_lon\n",
    "nc_file.rootgrp.time_coverage_start = min_time_str\n",
    "nc_file.rootgrp.time_coverage_end = max_time_str\n",
    "nc_file.rootgrp.time_coverage_duration = time_delta_str\n",
    "nc_file.rootgrp.time_coverage_resolution = time_resolution_str\n",
    "nc_file.rootgrp.id = file_name\n",
    "nc_file.rootgrp.date_created = utilcf.datetime2str(datetime.now(timezone.utc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc_file.rootgrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza dados para confirma se esta tudo ok.\n",
    "# Date time deve ser linear, sem quebras abruptas\n",
    "if IN_JUPYTER:\n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2)\n",
    "    ax0.plot(time[:])\n",
    "    ax0.set_title('Date time')\n",
    "    ax1.plot(precip[:])\n",
    "    ax1.set_title('Variavel')\n",
    "    plt.show() \n",
    "\n",
    "    # Mostra imagem\n",
    "    image = utilcf.base642bin(station_image[:])\n",
    "    print(station_image.file_name)\n",
    "    im = Image.open(image)\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha e salva arquivo\n",
    "nc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump do arquivo nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta o dump do arquivo netcdf\n",
    "# Precisa instalar: sudo apt install netcdf-bin \n",
    "if IN_JUPYTER:\n",
    "    cmd = '\"' + nc_file_path + '\"'\n",
    "    !ncdump {  cmd }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifica se arquivo .nc atende o CF Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica compatibilidade com CF\n",
    "# Precisa instalar pip install cfchecker \n",
    "# site: https://pypi.org/project/cfchecker/\n",
    "if IN_JUPYTER:\n",
    "    CF_VERSION = '1.7'\n",
    "    cmd = '-v ' + CF_VERSION + ' ' + '\"' + nc_file_path + '\"'\n",
    "    !cfchecks {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
